{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# 1. Explain the concept of forward propagation in a neural network.\n",
        "# Ans: Forward propagation is the process through which input data is passed through a neural network layer by layer to generate an output or prediction. It is the first phase in training or evaluating a neural network, followed by the backpropagation phase where the network learns by updating its weights.\n",
        "\n",
        "# Key Steps in Forward Propagation:\n",
        "# Input Layer:\n",
        "\n",
        "# The network receives input data, which is represented as a vector of features.\n",
        "# Example: If the input is an image, each pixel value would serve as an input feature.\n",
        "# Linear Transformation (Weights and Bias):\n",
        "\n",
        "# Each input feature is multiplied by its corresponding weight.\n",
        "# A bias term is added to the weighted sum to allow the model to shift the activation function as needed.\n",
        "\n",
        "# Non-Linear Activation:\n",
        "# The output of the linear transformation (z) is passed through an activation function to introduce non-linearity into the model.\n",
        "\n",
        "# Propagation Through Hidden Layers:\n",
        "\n",
        "# The activation output from one layer becomes the input for the next layer.\n",
        "# This process continues until the final layer is reached.\n",
        "\n",
        "# Output Layer:\n",
        "# The final layer produces the network’s output. Depending on the task:\n",
        "# Regression: Outputs a continuous value.\n",
        "# Classification: Outputs probabilities (using a softmax or sigmoid function)."
      ],
      "metadata": {
        "id": "4pG7vA1txbON"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. What is the purpose of the activation function in forward propagation?\n",
        "# Ans: The activation function in forward propagation introduces non-linearity into a neural network, allowing it to learn and model complex relationships between the input features and the target output. Without an activation function, the neural network would behave like a linear model, regardless of the number of layers, limiting its capability to solve non-linear problems.\n",
        "\n",
        "# Key Roles of the Activation Function:\n",
        "# Introduces Non-Linearity:\n",
        "\n",
        "# Real-world data and problems are often non-linear. Activation functions enable the network to approximate complex mappings from inputs to outputs by introducing non-linear transformations.\n",
        "# Example: Recognizing patterns like edges in images or detecting sentiment in text.\n",
        "# Enables Hierarchical Feature Learning:\n",
        "\n",
        "# Activation functions allow deeper layers in the network to learn progressively more abstract and complex features (e.g., from edges to shapes to objects in image classification tasks).\n",
        "# Controls the Flow of Information:\n",
        "\n",
        "# They decide whether a neuron should be activated (fire) or not, mimicking the behavior of biological neurons.\n",
        "# Adds Flexibility to the Model:\n",
        "\n",
        "# Activation functions allow the network to adapt to various types of data distributions and tasks.\n",
        "# Helps in Capturing Non-Linear Patterns: Without activation functions, stacking multiple layers would still result in a linear transformation due to the composition of linear functions being linear."
      ],
      "metadata": {
        "id": "oD0F2Eo4ynEp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Describe the steps involved in the backward propagation (backpropagation) algorithm.\n",
        "# Ans: The backpropagation algorithm is a method used in training neural networks to minimize the error (loss) by updating the weights and biases through gradient descent. It calculates the gradient of the loss function with respect to each parameter by propagating the error backward through the network.\n",
        "\n",
        "# Steps in Backpropagation:\n",
        "# 1. Forward Propagation:\n",
        "# Input data is passed through the network, and outputs are computed at each layer.\n",
        "# The final output is compared to the target to compute the loss using a loss function, e.g., Mean Squared Error (MSE) or Cross-Entropy.\n",
        "\n",
        "# 2. Compute Output Error (Loss Gradient w.r.t Output):\n",
        "# The derivative of the loss function with respect to the network’s output (y^) is computed.\n",
        "\n",
        "# 3. Backward Propagation of Errors:\n",
        "# The error is propagated backward through the network to compute gradients for each layer.\n",
        "# a. Compute Gradients for Output Layer:\n",
        "# Using the chain rule, calculate the derivative of the loss function with respect to weights and biases in the output layer.\n",
        "\n",
        "# 4. Update Weights and Biases:\n",
        "# Using gradient descent (or a variant like SGD, Adam, or RMSprop), weights and biases are updated to minimize the loss.\n",
        "\n",
        "# 5. Repeat for All Layers:\n",
        "# Steps 3 and 4 are repeated layer by layer, moving from the output layer back to the input layer.\n",
        "\n",
        "# 6. Iterate Over Multiple Epochs:\n",
        "# The entire forward and backward pass is repeated for multiple epochs (iterations over the training dataset) until the loss converges to a satisfactory level or stops decreasing significantly."
      ],
      "metadata": {
        "id": "TeuKho0AzJut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. What is the purpose of the chain rule in backpropagation?\n",
        "# Ans: The chain rule is a fundamental mathematical principle used in backpropagation to compute the gradient of a loss function with respect to the parameters (weights and biases) of a neural network. It allows the network to efficiently calculate how changes in parameters affect the final output and loss by breaking down the gradient computation into manageable steps.\n",
        "\n",
        "# Key Roles of the Chain Rule in Backpropagation:\n",
        "# Propagating Error Gradients Through Layers:\n",
        "\n",
        "# Neural networks consist of multiple layers, and the chain rule enables the computation of the loss gradient layer by layer, starting from the output and moving backward (hence \"backpropagation\").\n",
        "# Each layer's output depends on its weights, biases, and the previous layer's output. The chain rule links these dependencies.\n",
        "# Handling Composite Functions:\n",
        "\n",
        "# The output of each layer is a composite function of the inputs, weights, biases, and activation functions.\n",
        "# The chain rule allows gradients to be computed for such composite functions by decomposing the derivatives into the product of simpler derivatives.\n",
        "# Efficient Gradient Computation:\n",
        "\n",
        "# The chain rule breaks the gradient computation into smaller parts, allowing efficient computation through the chain of dependencies. This ensures gradients for deep networks can be calculated without redundancy.\n",
        "# Guiding Parameter Updates:\n",
        "\n",
        "# By determining how much each weight and bias contributes to the error, the chain rule provides the information needed to adjust these parameters during the optimization step (e.g., gradient descent).\n",
        "\n",
        "# Benefits of Using the Chain Rule:\n",
        "# Simplifies Complex Networks:\n",
        "\n",
        "# Neural networks with many layers involve intricate dependencies between inputs, weights, and outputs. The chain rule systematically handles these dependencies.\n",
        "# Enables Deep Learning:\n",
        "\n",
        "# Without the chain rule, computing gradients for deep networks would be intractable due to their complexity.\n",
        "# Efficient Computation:\n",
        "\n",
        "# The chain rule is central to the automatic differentiation techniques used in modern deep learning frameworks (e.g., TensorFlow, PyTorch).\n"
      ],
      "metadata": {
        "id": "rsPBGeIM0AOa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Implement the forward propagation process for a simple neural network with one hidden layer using NumPy.\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Define activation functions\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def softmax(x):\n",
        "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
        "\n",
        "# Forward propagation\n",
        "def forward_propagation(X, W1, b1, W2, b2):\n",
        "\n",
        "    # Hidden layer computation\n",
        "    Z1 = np.dot(X, W1) + b1\n",
        "    A1 = relu(Z1)\n",
        "\n",
        "    # Output layer computation\n",
        "    Z2 = np.dot(A1, W2) + b2\n",
        "    A2 = softmax(Z2)\n",
        "\n",
        "    return A2, (Z1, A1, Z2, A2)\n",
        "\n",
        "# Usage\n",
        "np.random.seed(42)\n",
        "\n",
        "# Sample input data (m samples, n_input features)\n",
        "X = np.array([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
        "\n",
        "# Network parameters\n",
        "n_input = 2\n",
        "n_hidden = 3\n",
        "n_output = 2\n",
        "\n",
        "# Initialize weights and biases\n",
        "W1 = np.random.randn(n_input, n_hidden)\n",
        "b1 = np.random.randn(1, n_hidden)\n",
        "W2 = np.random.randn(n_hidden, n_output)\n",
        "b2 = np.random.randn(1, n_output)\n",
        "\n",
        "# Perform forward propagation\n",
        "output, cache = forward_propagation(X, W1, b1, W2, b2)\n",
        "\n",
        "print(\"Output of the network:\\n\", output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "roXjH3vP0M8p",
        "outputId": "87cd0eae-4fa5-48d7-f62b-0223795a58df"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output of the network:\n",
            " [[9.95886284e-01 4.11371643e-03]\n",
            " [9.99929907e-01 7.00925198e-05]\n",
            " [9.99998592e-01 1.40782351e-06]]\n"
          ]
        }
      ]
    }
  ]
}